import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import torch
from transformers import CLIPProcessor, CLIPModel
import os
import glob
import pickle # ìºì‹±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- ì„¤ì • ---
MODEL_NAME = "openai/clip-vit-large-patch14"
IMAGE_DIR = "assets"
IMAGE_EXTENSIONS = ["*.jpg", "*.jpeg", "*.png", "*.bmp"]
TOP_N_RESULTS = 6
# ëª¨ë¸ì— ë§ì¶° ìƒˆë¡œìš´ ìºì‹œ íŒŒì¼ ì´ë¦„ ì •ì˜
CACHE_FILE = "embedding_cache_openai_large.pkl" 

# --- GPU ì„¤ì • ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ ì‚¬ìš©í•˜ëŠ” ì¥ì¹˜: {device.upper()}")

# --- ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ---
print(f"ğŸ¤– '{MODEL_NAME}' ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
try:
    model = CLIPModel.from_pretrained(MODEL_NAME).to(device)
    processor = CLIPProcessor.from_pretrained(MODEL_NAME)
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
except Exception as e:
    print(f"ğŸ”¥ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜, ëª¨ë¸ ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# --- [ìƒˆë¡œìš´ í•¨ìˆ˜] assets í´ë”ì˜ ìƒíƒœë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ---
def get_assets_state(paths):
    """í´ë”ì˜ í˜„ì¬ ìƒíƒœë¥¼ (íŒŒì¼ ê²½ë¡œ: ìµœì¢… ìˆ˜ì • ì‹œê°„) ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
    # ìˆœì„œê°€ ë³´ì¥ëœ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœ ìƒì„±
    return {path: os.path.getmtime(path) for path in paths if os.path.exists(path)}

# --- ì´ë¯¸ì§€ ì‚¬ì „ ì²˜ë¦¬ (ìºì‹± ë¡œì§ ì¶”ê°€) ---
# [!!! ì¶”ê°€ í•´ê²°ì±… !!!] ê²½ë¡œ êµ¬ë¶„ìë¥¼ '/'ë¡œ í†µì¼í•˜ì—¬ ì‹œìŠ¤í…œ ê°„ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°
raw_paths = [p for ext in IMAGE_EXTENSIONS for p in glob.glob(os.path.join(IMAGE_DIR, ext))]
# Windowsì˜ ë°±ìŠ¬ë˜ì‹œ(\)ë¥¼ í¬ì›Œë“œìŠ¬ë˜ì‹œ(/)ë¡œ ë³€í™˜í•˜ê³ , ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
image_paths = sorted([p.replace('\\', '/') for p in raw_paths])

image_embeddings = None
cache_valid = False

if os.path.exists(CACHE_FILE):
    try:
        print(f"â„¹ï¸ ìºì‹œ íŒŒì¼({CACHE_FILE})ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤...")
        with open(CACHE_FILE, "rb") as f:
            cached_data = pickle.load(f)
        
        current_state = get_assets_state(image_paths)
        
        if cached_data.get("paths") == image_paths and cached_data.get("state") == current_state:
            print("âœ… ìœ íš¨í•œ ìºì‹œ ë°œê²¬! ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            image_paths = cached_data["paths"]
            image_embeddings = cached_data["embeddings"].to(device)
            cache_valid = True
        else:
            print("âš ï¸ ìºì‹œê°€ í˜„ì¬ íŒŒì¼ ìƒíƒœì™€ ë‹¤ë¦…ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸ”¥ ìºì‹œ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

if not cache_valid:
    if image_paths:
        print(f"ğŸ–¼ï¸ ì´ {len(image_paths)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ì „ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        embeddings_list, valid_paths = [], []
        with torch.no_grad():
            for path in image_paths:
                try:
                    image = Image.open(path).convert("RGB")
                    inputs = processor(images=image, return_tensors="pt").to(device)
                    embedding = model.get_image_features(**inputs)
                    embeddings_list.append(embedding)
                    valid_paths.append(path)
                except Exception as e:
                    print(f"ê²½ê³ : '{path}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if embeddings_list:
            image_paths = valid_paths
            image_embeddings = torch.cat(embeddings_list)
            image_embeddings /= image_embeddings.norm(p=2, dim=-1, keepdim=True)
            print("âœ… ì´ë¯¸ì§€ ì‚¬ì „ ì²˜ë¦¬ ì™„ë£Œ!")

            print(f"ğŸ’¾ ìƒˆë¡œìš´ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œ íŒŒì¼({CACHE_FILE})ì— ì €ì¥í•©ë‹ˆë‹¤.")
            new_cache_data = {
                "state": get_assets_state(image_paths),
                "paths": image_paths, # ì •ê·œí™”ë˜ê³  ì •ë ¬ëœ ê²½ë¡œ ì €ì¥
                "embeddings": image_embeddings.cpu()
            }
            with open(CACHE_FILE, "wb") as f:
                pickle.dump(new_cache_data, f)
        else:
            image_paths, image_embeddings = [], torch.tensor([])
    else:
        image_paths, image_embeddings = [], torch.tensor([])

if image_embeddings is None or len(image_paths) == 0:
    print(f"ê²½ê³ : '{IMAGE_DIR}' í´ë”ì— ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    image_paths, image_embeddings = [], torch.tensor([])

# --- GUI ì• í”Œë¦¬ì¼€ì´ì…˜ ---
# (ì´í•˜ ì½”ë“œëŠ” ë³€ê²½í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.)
class ImageSearchApp:
    def __init__(self, root):
        self.root = root
        self.root.title("CLIP ì´ë¯¸ì§€ ê²€ìƒ‰ê¸°")
        self.root.geometry("800x600")

        # ê²€ìƒ‰ í”„ë ˆì„
        search_frame = ttk.Frame(root, padding="10")
        search_frame.pack(fill=tk.X)
        ttk.Label(search_frame, text="ê²€ìƒ‰ì–´ ì…ë ¥:").pack(side=tk.LEFT, padx=5)
        self.search_entry = ttk.Entry(search_frame, width=50)
        self.search_entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        self.search_entry.bind("<Return>", self.search_images)
        self.search_button = ttk.Button(search_frame, text="ğŸ” ê²€ìƒ‰", command=self.search_images)
        self.search_button.pack(side=tk.LEFT, padx=5)

        # ê²°ê³¼ í”„ë ˆì„
        self.result_frame = ttk.Frame(root, padding="10")
        self.result_frame.pack(fill=tk.BOTH, expand=True)

    def search_images(self, event=None):
        if len(image_paths) == 0:
            print("ê²€ìƒ‰í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            from tkinter import messagebox
            messagebox.showinfo("ì•Œë¦¼", "assets í´ë”ì— ê²€ìƒ‰í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        query = self.search_entry.get()
        if not query:
            return

        print(f"\nğŸ” ê²€ìƒ‰ì–´ '{query}'ë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for widget in self.result_frame.winfo_children():
            widget.destroy()

        with torch.no_grad():
            text_inputs = processor(text=query, return_tensors="pt").to(device)
            text_embedding = model.get_text_features(**text_inputs)
            text_embedding /= text_embedding.norm(p=2, dim=-1, keepdim=True)

        similarities = (text_embedding @ image_embeddings.T).squeeze(0)
        top_indices = similarities.argsort(descending=True)[:TOP_N_RESULTS]

        print(f"âœ¨ ìƒìœ„ {len(top_indices)}ê°œ ê²°ê³¼:")
        for i, idx in enumerate(top_indices):
            path, score = image_paths[idx], similarities[idx].item()
            print(f"  - {i+1}ìœ„: {path} (ìœ ì‚¬ë„: {score:.4f})")

            img = Image.open(path)
            img.thumbnail((200, 200))
            photo = ImageTk.PhotoImage(img)

            item_frame = ttk.Frame(self.result_frame)
            img_label = ttk.Label(item_frame, image=photo)
            img_label.image = photo
            img_label.pack()
            score_label = ttk.Label(item_frame, text=f"ìœ ì‚¬ë„: {score:.3f}")
            score_label.pack()
            item_frame.grid(row=i // 3, column=i % 3, padx=10, pady=10)

if __name__ == "__main__":
    root = tk.Tk()
    app = ImageSearchApp(root)
    root.mainloop()

